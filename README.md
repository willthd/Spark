## Spark 기초 개념 및 특징

* Transformation의 처리 과정을 정의하는 분산 프로그래밍 모델이다.
* 스파크 Dataframe은 수천 대의 컴퓨터에 분산되어 있다.(Python과 R은 일반적으로 분산 컴퓨터가 아닌 단일 컴퓨터에 존재)
* 스파크는 모든 익스큐터가 병렬로 작업을 수행할 수 있도록 파티션이라 불리는 청크 단위로 데이터를 분할한다.
* Transformation은 DataFrame을 변경하기 위한 명령이다. Transformation은 좁은 의존성과 넓은 의존성 두 가지 유형이 있다. 좁은 의존성은 각 입력 파티션이 하나의 출력 파티션에만 영향을 주며 모든 작업이 메모리에서 일어난다. 넓은 의존성은 하나의 입력 파티션이 여러 출력 파티션에 영향을 미치는 것을 의미하며 디스크에 저장한다.
* **셔플(shuffle)** : 스파크가 클러스터에서 파티션을 교환하는 것. 스파크는 셔플의 결과를 메모리가 아니라 디스크에 저장한다.
* 스파크는 셔플 수행 시 기본적으로 200개의 셔플 파티션을 생성한다.
* <u>스파크는 해당 DataFrame이나 자신의 원본 DataFrame에 액션이 호출되기 전까지 데이터를 읽지 않는다. 액션을 지정하면 스파크 잡이 시작된다.</u> 이렇듯 스파크가 연산 그래프를 처리하기 직전까지 기다리는 동작 방식을 **지연 연산**이라고 한다.
* createOrReplaceTempView 메서드를 호출하면 모든 DataFrame을 테이블이나 뷰로 만들 수 있다.(SQL로 접근 가능해짐)
* 스파크는 SQL 쿼리를 DataFrame 코드와 같은 샐행 계획으로 컴파일하므로 둘 사이의 성능 차이는 없다.

* RDD(Resilient Distributed Dataset) : 스파크에서 정의한 분산 데이터 모델인데 내부에는 단위 데이터를 포함하고 있고 저장할 때는 여러 서버에 나누어 저장되며, 처리할 때는 각 서버에 저장된 데이터를 동시에 병렬로 처리할 수 있는 모델이다. 스파크의 거의 모든 기능은 RDD를 기반으로 만들어졌으며, DataFrame 연산도 RDD를 기반으로 만들어져 저수준 명령으로 컴파일된다.

</br>

## 구조적 API

### DataFrame과 Dataset 비교

본질적으로 구조적 API에는 '비타입형'인 DataFrame과 '타입형'인 Dataset이 있다. DataFrame은 스키마에 명시된 데이터 타입ㅇ의 일치 여부를 **런타임**이 되어서 확인하고, Dataset은 **컴파일 타임**에 확인한다. 스파킁의 DataFrae은 Row타입으로 구성된 Dataset이다. Row타입은 스파크가 사용하는 '연산에 최적화된 인메모리 포맷'의 내부적인 표현 방식이다.